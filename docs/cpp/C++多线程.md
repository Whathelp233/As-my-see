# C++å¤šçº¿ç¨‹ç¼–ç¨‹æ·±åº¦æŒ‡å—
> æ–‡æ¡£çŠ¶æ€: å…¨é¢ä¼˜åŒ–ç‰ˆæœ¬
> æ›´æ–°æ—¶é—´: 2026å¹´02æœˆ27æ—¥
> æ¶µç›–C++11åˆ°C++23çš„å¤šçº¿ç¨‹ç‰¹æ€§

## ğŸ“– æ¦‚è¿°

C++å¤šçº¿ç¨‹æ˜¯ç°ä»£C++ç¼–ç¨‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç”¨äºå……åˆ†åˆ©ç”¨å¤šæ ¸CPUæ€§èƒ½ï¼Œæé«˜ç¨‹åºå¹¶å‘å¤„ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§åº”ç”¨ï¼Œå…¨é¢ä»‹ç»C++å¤šçº¿ç¨‹ç¼–ç¨‹ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### 1. å¹¶è¡Œä¸å¹¶å‘
- **å¹¶è¡Œ**: å¤šä¸ªä»»åŠ¡åŒæ—¶æ‰§è¡Œï¼ˆéœ€è¦å¤šæ ¸CPUï¼‰
- **å¹¶å‘**: å¤šä¸ªä»»åŠ¡äº¤æ›¿æ‰§è¡Œï¼Œçœ‹èµ·æ¥åŒæ—¶è¿›è¡Œ

### 2. çº¿ç¨‹ vs è¿›ç¨‹
| ç‰¹æ€§ | çº¿ç¨‹ | è¿›ç¨‹ |
|------|------|------|
| èµ„æºå¼€é”€ | å° | å¤§ |
| é€šä¿¡æ–¹å¼ | å…±äº«å†…å­˜ | IPCï¼ˆç®¡é“ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰ï¼‰ |
| åˆ›å»ºé€Ÿåº¦ | å¿« | æ…¢ |
| ç‹¬ç«‹æ€§ | ä½ï¼ˆå…±äº«åœ°å€ç©ºé—´ï¼‰ | é«˜ï¼ˆç‹¬ç«‹åœ°å€ç©ºé—´ï¼‰ |

### 3. çº¿ç¨‹å®‰å…¨
- **ç«æ€æ¡ä»¶**: å¤šä¸ªçº¿ç¨‹è®¿é—®å…±äº«æ•°æ®ï¼Œç»“æœä¾èµ–äºæ‰§è¡Œé¡ºåº
- **æ•°æ®ç«äº‰**: å¤šä¸ªçº¿ç¨‹åŒæ—¶ä¿®æ”¹åŒä¸€æ•°æ®
- **æ­»é”**: å¤šä¸ªçº¿ç¨‹äº’ç›¸ç­‰å¾…å¯¹æ–¹é‡Šæ”¾èµ„æº

## ğŸš€ ç°ä»£C++å¤šçº¿ç¨‹ç‰¹æ€§

### C++11 åŸºç¡€
```cpp
#include <thread>
#include <iostream>

void hello() {
    std::cout << "Hello from thread!" << std::endl;
}

int main() {
    std::thread t(hello);  // åˆ›å»ºçº¿ç¨‹
    t.join();              // ç­‰å¾…çº¿ç¨‹ç»“æŸ
    return 0;
}
```

### C++14/17 å¢å¼º
- `std::shared_timed_mutex`: è¯»å†™é”
- `std::shared_lock`: å…±äº«é”
- å¹¶è¡Œç®—æ³•: `std::for_each` å¹¶è¡Œç‰ˆæœ¬

### C++20/23 æœ€æ–°ç‰¹æ€§
```cpp
#include <thread>
#include <stop_token>
#include <latch>

// C++20: è‡ªåŠ¨joinçš„çº¿ç¨‹
std::jthread worker([](std::stop_token stoken) {
    while (!stoken.stop_requested()) {
        // æ‰§è¡Œä»»åŠ¡
        std::this_thread::sleep_for(100ms);
    }
});

// C++20: é—¨é—©åŒæ­¥
std::latch completion_latch(3);  // ç­‰å¾…3ä¸ªä»»åŠ¡

void task() {
    // æ‰§è¡Œä»»åŠ¡
    completion_latch.count_down();  // å®Œæˆä»»åŠ¡
}
```

## ğŸ”§ åŒæ­¥æœºåˆ¶è¯¦è§£

### 1. äº’æ–¥é” (Mutex)
```cpp
#include <mutex>
#include <vector>

std::mutex mtx;
std::vector<int> shared_data;

void safe_push(int value) {
    std::lock_guard<std::mutex> lock(mtx);  // è‡ªåŠ¨åŠ é”è§£é”
    shared_data.push_back(value);
}

// ä½¿ç”¨unique_lockï¼ˆæ›´çµæ´»ï¼‰
void complex_operation() {
    std::unique_lock<std::mutex> lock(mtx, std::defer_lock);
    // åšä¸€äº›ä¸æ¶‰åŠå…±äº«æ•°æ®çš„æ“ä½œ...
    lock.lock();  // éœ€è¦æ—¶å†åŠ é”
    // æ“ä½œå…±äº«æ•°æ®...
    lock.unlock();  // å¯ä»¥æå‰è§£é”
}
```

### 2. æ¡ä»¶å˜é‡ (Condition Variable)
```cpp
#include <condition_variable>
#include <queue>

std::mutex mtx;
std::condition_variable cv;
std::queue<int> data_queue;
bool producer_done = false;

void producer() {
    for (int i = 0; i < 10; ++i) {
        std::this_thread::sleep_for(100ms);
        {
            std::lock_guard<std::mutex> lock(mtx);
            data_queue.push(i);
        }
        cv.notify_one();  // é€šçŸ¥ä¸€ä¸ªæ¶ˆè´¹è€…
    }
    {
        std::lock_guard<std::mutex> lock(mtx);
        producer_done = true;
    }
    cv.notify_all();  // é€šçŸ¥æ‰€æœ‰æ¶ˆè´¹è€…
}

void consumer() {
    while (true) {
        std::unique_lock<std::mutex> lock(mtx);
        // ç­‰å¾…æ¡ä»¶ï¼šé˜Ÿåˆ—ä¸ä¸ºç©ºæˆ–ç”Ÿäº§è€…å·²å®Œæˆ
        cv.wait(lock, []{ 
            return !data_queue.empty() || producer_done; 
        });
        
        if (data_queue.empty() && producer_done) break;
        
        int value = data_queue.front();
        data_queue.pop();
        lock.unlock();
        
        process(value);  // å¤„ç†æ•°æ®
    }
}
```

### 3. åŸå­æ“ä½œ (Atomic)
```cpp
#include <atomic>
#include <thread>

std::atomic<int> counter{0};

void increment(int n) {
    for (int i = 0; i < n; ++i) {
        // å†…å­˜é¡ºåºï¼šæ”¾æ¾é¡ºåºï¼Œæ€§èƒ½æœ€é«˜
        counter.fetch_add(1, std::memory_order_relaxed);
    }
}

// å†…å­˜å±éšœç¤ºä¾‹
std::atomic<bool> ready{false};
int data = 0;

void writer() {
    data = 42;
    ready.store(true, std::memory_order_release);  // é‡Šæ”¾å±éšœ
}

void reader() {
    while (!ready.load(std::memory_order_acquire)) {  // è·å–å±éšœ
        std::this_thread::yield();
    }
    // è¿™é‡Œä¿è¯çœ‹åˆ° data = 42
    std::cout << "Data: " << data << std::endl;
}
```

## ğŸ—ï¸ é«˜çº§å¹¶å‘æ¨¡å¼

### 1. çº¿ç¨‹æ± å®ç°
```cpp
#include <vector>
#include <thread>
#include <queue>
#include <functional>
#include <future>

class ThreadPool {
private:
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex queue_mutex;
    std::condition_variable condition;
    bool stop = false;
    
public:
    ThreadPool(size_t threads = std::thread::hardware_concurrency()) {
        for (size_t i = 0; i < threads; ++i) {
            workers.emplace_back([this] {
                while (true) {
                    std::function<void()> task;
                    {
                        std::unique_lock<std::mutex> lock(this->queue_mutex);
                        this->condition.wait(lock, [this] {
                            return this->stop || !this->tasks.empty();
                        });
                        if (this->stop && this->tasks.empty()) return;
                        task = std::move(this->tasks.front());
                        this->tasks.pop();
                    }
                    task();
                }
            });
        }
    }
    
    template<class F, class... Args>
    auto enqueue(F&& f, Args&&... args) 
        -> std::future<typename std::result_of<F(Args...)>::type> {
        using return_type = typename std::result_of<F(Args...)>::type;
        
        auto task = std::make_shared<std::packaged_task<return_type()>>(
            std::bind(std::forward<F>(f), std::forward<Args>(args)...)
        );
        
        std::future<return_type> res = task->get_future();
        {
            std::unique_lock<std::mutex> lock(queue_mutex);
            if (stop) throw std::runtime_error("enqueue on stopped ThreadPool");
            tasks.emplace([task](){ (*task)(); });
        }
        condition.notify_one();
        return res;
    }
    
    ~ThreadPool() {
        {
            std::unique_lock<std::mutex> lock(queue_mutex);
            stop = true;
        }
        condition.notify_all();
        for (std::thread &worker : workers)
            worker.join();
    }
};

// ä½¿ç”¨ç¤ºä¾‹
ThreadPool pool(4);
auto future = pool.enqueue([](int x) { return x * x; }, 10);
int result = future.get();  // ç­‰å¾…å¹¶è·å–ç»“æœ
```

### 2. æ— é”é˜Ÿåˆ—
```cpp
template<typename T>
class LockFreeQueue {
private:
    struct Node {
        T data;
        std::atomic<Node*> next;
        Node(const T& data) : data(data), next(nullptr) {}
    };
    
    std::atomic<Node*> head;
    std::atomic<Node*> tail;
    
public:
    LockFreeQueue() {
        Node* dummy = new Node(T());
        head.store(dummy);
        tail.store(dummy);
    }
    
    ~LockFreeQueue() {
        while (Node* old_head = head.load()) {
            head.store(old_head->next);
            delete old_head;
        }
    }
    
    void enqueue(const T& value) {
        Node* new_node = new Node(value);
        Node* old_tail = tail.load();
        
        while (true) {
            Node* next = old_tail->next.load();
            if (next == nullptr) {
                if (old_tail->next.compare_exchange_weak(next, new_node)) {
                    tail.compare_exchange_strong(old_tail, new_node);
                    return;
                }
            } else {
                tail.compare_exchange_strong(old_tail, next);
            }
        }
    }
    
    bool dequeue(T& result) {
        Node* old_head = head.load();
        while (true) {
            Node* next = old_head->next.load();
            if (next == nullptr) return false;
            
            if (head.compare_exchange_weak(old_head, next)) {
                result = next->data;
                delete old_head;
                return true;
            }
        }
    }
};
```

### 3. å·¥ä½œçªƒå–é˜Ÿåˆ—
```cpp
class WorkStealingQueue {
private:
    typedef std::function<void()> Task;
    std::deque<Task> tasks;
    mutable std::mutex mutex;
    
public:
    void push(Task task) {
        std::lock_guard<std::mutex> lock(mutex);
        tasks.push_front(std::move(task));
    }
    
    bool try_pop(Task& task) {
        std::lock_guard<std::mutex> lock(mutex);
        if (tasks.empty()) return false;
        task = std::move(tasks.front());
        tasks.pop_front();
        return true;
    }
    
    bool try_steal(Task& task) {
        std::lock_guard<std::mutex> lock(mutex);
        if (tasks.empty()) return false;
        task = std::move(tasks.back());
        tasks.pop_back();
        return true;
    }
    
    bool empty() const {
        std::lock_guard<std::mutex> lock(mutex);
        return tasks.empty();
    }
};
```

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. é¿å…è™šå‡å…±äº« (False Sharing)
```cpp
// é”™è¯¯ï¼šå¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ç¼“å­˜è¡Œ
struct Data {
    int x;  // çº¿ç¨‹1é¢‘ç¹è®¿é—®
    int y;  // çº¿ç¨‹2é¢‘ç¹è®¿é—®
};

// æ­£ç¡®ï¼šç¼“å­˜è¡Œå¯¹é½
struct alignas(64) AlignedData {  // 64å­—èŠ‚ï¼ˆå…¸å‹ç¼“å­˜è¡Œå¤§å°ï¼‰
    int x;
    char padding[60];  // å¡«å……
};

// çº¿ç¨‹å±€éƒ¨æ•°æ®
struct ThreadLocalData {
    alignas(64) int local_counter;
    alignas(64) double local_sum;
    // ... å…¶ä»–çº¿ç¨‹å±€éƒ¨æ•°æ®
};
```

### 2. å‡å°‘é”ç«äº‰
```cpp
// ç»†ç²’åº¦é”
class FineGrainedCounter {
private:
    struct Bucket {
        std::mutex mutex;
        int count = 0;
    };
    
    std::vector<Bucket> buckets;
    
public:
    FineGrainedCounter(size_t bucket_count = 16) : buckets(bucket_count) {}
    
    void increment(size_t thread_id) {
        auto& bucket = buckets[thread_id % buckets.size()];
        std::lock_guard<std::mutex> lock(bucket.mutex);
        ++bucket.count;
    }
    
    int total() const {
        int sum = 0;
        for (const auto& bucket : buckets) {
            std::lock_guard<std::mutex> lock(bucket.mutex);
            sum += bucket.count;
        }
        return sum;
    }
};
```

### 3. ä½¿ç”¨çº¿ç¨‹å±€éƒ¨å­˜å‚¨
```cpp
thread_local int thread_specific_value = 0;

void process() {
    // æ¯ä¸ªçº¿ç¨‹æœ‰è‡ªå·±çš„å‰¯æœ¬
    ++thread_specific_value;
    
    // è®¿é—®å…¨å±€æ•°æ®æ—¶éœ€è¦åŒæ­¥
    static std::atomic<int> global_counter{0};
    global_counter.fetch_add(1, std::memory_order_relaxed);
}
```

## ğŸ” è°ƒè¯•ä¸æµ‹è¯•

### 1. çº¿ç¨‹å®‰å…¨æµ‹è¯•
```cpp
#include <gtest/gtest.h>
#include <thread>
#include <vector>

TEST(ThreadSafeTest, ConcurrentIncrement) {
    const int num_threads = 10;
    const int increments_per_thread = 1000;
    
    std::atomic<int> counter{0};
    std::vector<std::thread> threads;
    
    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back([&counter, increments_per_thread]() {
            for (int j = 0; j < increments_per_thread; ++j) {
                counter.fetch_add(1);
            }
        });
    }
    
    for (auto& t : threads) {
        t.join();
    }
    
    EXPECT_EQ(counter.load(), num_threads * increments_per_thread);
}
```

### 2. æ­»é”æ£€æµ‹
```cpp
#include <mutex>

std::mutex mtx1, mtx2;

// å®‰å…¨æ–¹å¼ï¼šæŒ‰å›ºå®šé¡ºåºåŠ é”
void safe_operation() {
    std::lock(mtx1, mtx2);  // åŒæ—¶é”å®šï¼Œé¿å…æ­»é”
    std::lock_guard<std::mutex> lock1(mtx1, std::adopt_lock);
    std::lock_guard<std::mutex> lock2(mtx2, std::adopt_lock);
    // æ“ä½œå…±äº«èµ„æº
}

// ä½¿ç”¨std::scoped_lockï¼ˆC++17ï¼‰
void modern_safe_operation() {
    std::scoped_lock lock(mtx1, mtx2);  // è‡ªåŠ¨å¤„ç†åŠ é”é¡ºåº
    // æ“ä½œå…±äº«èµ„æº
}
```

### 3. æ€§èƒ½åˆ†æå·¥å…·
- **ThreadSanitizer**: æ£€æµ‹æ•°æ®ç«äº‰
- **Helgrind**: Valgrindçš„çº¿ç¨‹é”™è¯¯æ£€æµ‹å·¥å…·
- **perf**: Linuxæ€§èƒ½åˆ†æ
- **Intel VTune**: ä¸“ä¸šæ€§èƒ½åˆ†æå·¥å…·

## ğŸ“Š å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå¹¶è¡Œæ•°æ®å¤„ç†
```cpp
template<typename InputIt, typename OutputIt, typename Func>
void parallel_transform(InputIt first, InputIt last, OutputIt d_first, 
                       Func f, size_t num_threads) {
    auto length = std::distance(first, last);
    if (length == 0) return;
    
    size_t block_size = (length + num_threads - 1) / num_threads;
    std::vector<std::thread> threads;
    
    for (size_t i = 0; i < num_threads; ++i) {
        InputIt start = first + i * block_size;
        InputIt end = first + std::min((i + 1) * block_size, 
                                      static_cast<size_t>(length));
        OutputIt dest = d_first + i * block_size;
        
        if (start < end) {
            threads.emplace_back([start, end, dest, &f]() {
                std::transform(start, end, dest, f);
            });
        }
    }
    
    for (auto& t : threads) {
        t.join();
    }
}

// ä½¿ç”¨ç¤ºä¾‹ï¼šå¹¶è¡Œå›¾åƒå¤„ç†
void parallel_image_filter(const std::vector<Pixel>& input, 
                          std::vector<Pixel>& output) {
    auto filter = [](Pixel p) {
        // åº”ç”¨å›¾åƒæ»¤é•œ
        p.r = 255 - p.r;  // åè‰²
        p.g = 255 - p.g;
        p.b = 255 - p.b;
        return p;
    };
    
    unsigned int num_threads = std::thread::hardware_concurrency();
    parallel_transform(input.begin(), input.end(), output.begin(), 
                      filter, num_threads);
}
```

### æ¡ˆä¾‹2ï¼šé«˜æ€§èƒ½WebæœåŠ¡å™¨
```cpp
class AsyncHTTPServer {
    ThreadPool pool;
    std::atomic<int> active_connections{0};
    
public:
    AsyncHTTPServer(size_t thread_count = 16) : pool(thread_count) {}
    
    void handle_request(int client_socket) {
        active_connections.fetch_add(1);
        
        pool.enqueue([this, client_socket]() {
            try {
                // è§£æHTTPè¯·æ±‚
                HttpRequest request = parse_request(client_socket);
                
                // å¤„ç†è¯·æ±‚
                HttpResponse response = process_request(request);
                
                // å‘é€å“åº”
                send_response(client_socket, response);
            } catch (const std::exception& e) {
                log_error("Request failed: " + std::string(e.what()));
                send_error_response(client_socket, 500);
            }
            
            close(client_socket);
            active_connections.fetch_sub(1);
        });
    }
    
    int get_active_connections() const {
        return active_connections.load();
    }
};
```

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [C++ Concurrency Reference](https://en.cppreference.com/w/cpp/thread)
- [C++ Memory Model](https://en.cppreference.com/w/cpp/atomic/memory_order)
- [C++20 Concurrency Features](https://en.cppreference.com/w/cpp/thread/jthread)

### æ¨èä¹¦ç±
1. **ã€ŠC++ Concurrency in Action (2nd Edition)ã€‹** - Anthony Williams
2. **ã€ŠThe Art of Multiprocessor Programmingã€‹** - Maurice Herlihy
3. **ã€ŠEffective Modern C++ã€‹** - Scott Meyers
4. **ã€ŠC++ High Performanceã€‹** - BjÃ¶rn Andrist, Viktor Sehr

### åœ¨çº¿è¯¾ç¨‹
- [C++ Concurrency on Coursera](https://www.coursera.org/learn/cpp-concurrency)
- [Modern C++ Concurrency on Udemy](https://www.udemy.com/course/modern-cpp-concurrency/)
- [CppCon Concurrency Talks](https://www.youtube.com/results?search_query=cppcon+concurrency)

### å·¥å…·å’Œåº“
- **Intel TBB**: çº¿ç¨‹æ„å»ºå—åº“
- **OpenMP**: å¹¶è¡Œç¼–ç¨‹API
- **Boost.Thread**: è·¨å¹³å°çº¿ç¨‹åº“
- **HPX**: é«˜æ€§èƒ½å¹¶è¡Œè¿è¡Œæ—¶

## ğŸ¯ å®é™…é¡¹ç›®åº”ç”¨

### æ¡ˆä¾‹1ï¼šé«˜æ€§èƒ½æ•°æ®å¤„ç†ç®¡é“
```cpp
// æ•°æ®å¤„ç†å™¨ç®¡é“
class DataPipeline {
    std::vector<std::thread> workers;
    std::vector<BlockingQueue<std::vector<Data>>> queues;
    
public:
    DataPipeline(size_t stages) {
        queues.resize(stages + 1);
        
        // åˆ›å»ºå¤„ç†é˜¶æ®µ
        for (size_t i = 0; i < stages; ++i) {
            workers.emplace_back([this, i] {
                process_stage(i);
            });
        }
    }
    
    void process_stage(size_t stage_idx) {
        auto& input_queue = queues[stage_idx];
        auto& output_queue = queues[stage_idx + 1];
        
        while (true) {
            auto data = input_queue.pop();
            if (data.is_poison_pill()) break;
            
            // å¤„ç†æ•°æ®
            process_data(data);
            
            // ä¼ é€’ç»™ä¸‹ä¸€é˜¶æ®µ
            output_queue.push(std::move(data));
        }
    }
    
    void push_data(std::vector<Data> data) {
        queues[0].push(std::move(data));
    }
    
    std::vector<Data> get_result() {
        return queues.back().pop();
    }
    
    ~DataPipeline() {
        // å‘é€æ¯’ä¸¸ä¿¡å·åœæ­¢æ‰€æœ‰çº¿ç¨‹
        for (auto& queue : queues) {
            queue.push_poison_pill();
        }
        
        for (auto& worker : workers) {
            if (worker.joinable()) worker.join();
        }
    }
};
```

### æ¡ˆä¾‹2ï¼šå®æ—¶æ¸¸æˆå¼•æ“
```cpp
// æ¸¸æˆå¼•æ“ä»»åŠ¡è°ƒåº¦å™¨
class GameTaskScheduler {
    struct Task {
        std::function<void()> func;
        std::chrono::steady_clock::time_point deadline;
        int priority;
        
        bool operator<(const Task& other) const {
            if (priority != other.priority) 
                return priority < other.priority;
            return deadline > other.deadline;  // æ›´æ—©çš„æˆªæ­¢æ—¶é—´ä¼˜å…ˆ
        }
    };
    
    std::priority_queue<Task> task_queue;
    std::mutex queue_mutex;
    std::condition_variable cv;
    std::vector<std::jthread> workers;
    std::atomic<bool> running{true};
    
public:
    GameTaskScheduler(size_t num_workers) {
        for (size_t i = 0; i < num_workers; ++i) {
            workers.emplace_back([this] {
                worker_loop();
            });
        }
    }
    
    void submit(std::function<void()> task, int priority = 0, 
                std::chrono::milliseconds delay = 0ms) {
        auto deadline = std::chrono::steady_clock::now() + delay;
        
        {
            std::lock_guard lock(queue_mutex);
            task_queue.push({std::move(task), deadline, priority});
        }
        
        cv.notify_one();
    }
    
    void worker_loop() {
        while (running) {
            std::unique_lock lock(queue_mutex);
            
            // ç­‰å¾…ä»»åŠ¡æˆ–è¶…æ—¶
            if (task_queue.empty()) {
                cv.wait(lock);
                continue;
            }
            
            auto next_task = task_queue.top();
            auto now = std::chrono::steady_clock::now();
            
            if (next_task.deadline > now) {
                // ä»»åŠ¡è¿˜æœªåˆ°æ‰§è¡Œæ—¶é—´
                cv.wait_until(lock, next_task.deadline);
                continue;
            }
            
            task_queue.pop();
            lock.unlock();
            
            // æ‰§è¡Œä»»åŠ¡
            try {
                next_task.func();
            } catch (const std::exception& e) {
                log_error("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: " + std::string(e.what()));
            }
        }
    }
    
    ~GameTaskScheduler() {
        running = false;
        cv.notify_all();
    }
};

// ä½¿ç”¨ç¤ºä¾‹ï¼šæ¸¸æˆå¸§æ›´æ–°
void update_game_frame(GameTaskScheduler& scheduler, GameWorld& world) {
    // å¹¶è¡Œæ›´æ–°ä¸åŒç³»ç»Ÿ
    scheduler.submit([&world] { world.update_physics(); }, 10);
    scheduler.submit([&world] { world.update_ai(); }, 5);
    scheduler.submit([&world] { world.update_animation(); }, 3);
    scheduler.submit([&world] { world.update_particles(); }, 1);
    
    // é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼šè¾“å…¥å¤„ç†
    scheduler.submit([&world] { world.process_input(); }, 100);
}
```

## ğŸ” é«˜çº§è°ƒè¯•æŠ€å·§

### 1. è‡ªå®šä¹‰è°ƒè¯•å·¥å…·
```cpp
// çº¿ç¨‹æ„ŸçŸ¥çš„è°ƒè¯•å™¨
class ThreadAwareDebugger {
    struct ThreadInfo {
        std::thread::id id;
        std::string name;
        std::chrono::steady_clock::time_point start_time;
        std::atomic<int> lock_count{0};
    };
    
    static inline thread_local ThreadInfo* current_thread = nullptr;
    static inline std::unordered_map<std::thread::id, ThreadInfo> threads;
    static inline std::mutex threads_mutex;
    
public:
    static void register_thread(const std::string& name = "") {
        std::lock_guard lock(threads_mutex);
        auto id = std::this_thread::get_id();
        threads[id] = {id, name.empty() ? "thread_" + std::to_string(id.hash()) : name,
                      std::chrono::steady_clock::now()};
        current_thread = &threads[id];
    }
    
    static void lock_acquired() {
        if (current_thread) {
            current_thread->lock_count.fetch_add(1);
        }
    }
    
    static void lock_released() {
        if (current_thread) {
            current_thread->lock_count.fetch_sub(1);
        }
    }
    
    static void dump_threads() {
        std::lock_guard lock(threads_mutex);
        
        std::cout << "\n=== çº¿ç¨‹çŠ¶æ€æŠ¥å‘Š ===\n";
        for (const auto& [id, info] : threads) {
            auto duration = std::chrono::steady_clock::now() - info.start_time;
            auto seconds = std::chrono::duration_cast<std::chrono::seconds>(duration);
            
            std::cout << "çº¿ç¨‹: " << info.name 
                      << " (ID: " << id << ")\n"
                      << "  è¿è¡Œæ—¶é—´: " << seconds.count() << "ç§’\n"
                      << "  æŒæœ‰é”æ•°: " << info.lock_count.load() << "\n"
                      << "  çŠ¶æ€: " << (info.lock_count > 0 ? "é”å®šä¸­" : "è¿è¡Œä¸­") << "\n\n";
        }
    }
};

// åŒ…è£…äº’æ–¥é”ä»¥è‡ªåŠ¨è·Ÿè¸ª
class DebugMutex {
    std::mutex mtx;
    
public:
    void lock() {
        mtx.lock();
        ThreadAwareDebugger::lock_acquired();
    }
    
    void unlock() {
        ThreadAwareDebugger::lock_released();
        mtx.unlock();
    }
    
    bool try_lock() {
        if (mtx.try_lock()) {
            ThreadAwareDebugger::lock_acquired();
            return true;
        }
        return false;
    }
};
```

### 2. æ€§èƒ½åˆ†æè¾…åŠ©
```cpp
// å¹¶å‘æ€§èƒ½åˆ†æå™¨
class ConcurrencyProfiler {
    struct ProfilePoint {
        std::string name;
        std::chrono::nanoseconds total_time{0};
        std::atomic<int64_t> call_count{0};
        std::atomic<int64_t> concurrent_calls{0};
        std::atomic<int64_t> max_concurrent{0};
    };
    
    static inline std::unordered_map<std::string, ProfilePoint> points;
    static inline std::mutex points_mutex;
    
    class ScopedProfile {
        ProfilePoint& point;
        std::chrono::steady_clock::time_point start;
        
    public:
        ScopedProfile(const std::string& name) 
            : point(get_point(name))
            , start(std::chrono::steady_clock::now()) {
            
            auto concurrent = point.concurrent_calls.fetch_add(1) + 1;
            auto max = point.max_concurrent.load();
            
            while (concurrent > max && 
                   !point.max_concurrent.compare_exchange_weak(max, concurrent)) {
                // æ›´æ–°æœ€å¤§å¹¶å‘æ•°
            }
        }
        
        ~ScopedProfile() {
            auto end = std::chrono::steady_clock::now();
            auto duration = end - start;
            
            point.total_time += duration;
            point.call_count.fetch_add(1);
            point.concurrent_calls.fetch_sub(1);
        }
        
    private:
        ProfilePoint& get_point(const std::string& name) {
            std::lock_guard lock(points_mutex);
            return points.try_emplace(name, ProfilePoint{name}).first->second;
        }
    };
    
public:
    static auto profile(const std::string& name) {
        return ScopedProfile(name);
    }
    
    static void report() {
        std::lock_guard lock(points_mutex);
        
        std::cout << "\n=== å¹¶å‘æ€§èƒ½æŠ¥å‘Š ===\n";
        for (const auto& [name, point] : points) {
            auto avg_time = point.call_count > 0 
                ? point.total_time.count() / point.call_count 
                : 0;
            
            std::cout << "å‡½æ•°: " << name << "\n"
                      << "  è°ƒç”¨æ¬¡æ•°: " << point.call_count << "\n"
                      << "  æ€»è€—æ—¶: " << point.total_time.count() << " ns\n"
                      << "  å¹³å‡è€—æ—¶: " << avg_time << " ns\n"
                      << "  æœ€å¤§å¹¶å‘: " << point.max_concurrent << "\n\n";
        }
    }
};

// ä½¿ç”¨ç¤ºä¾‹
void process_data_concurrently() {
    auto profile = ConcurrencyProfiler::profile("process_data");
    
    // å¹¶å‘å¤„ç†
    std::vector<std::thread> threads;
    for (int i = 0; i < 4; ++i) {
        threads.emplace_back([] {
            auto thread_profile = ConcurrencyProfiler::profile("worker_thread");
            // å·¥ä½œä»£ç ...
            std::this_thread::sleep_for(10ms);
        });
    }
    
    for (auto& t : threads) t.join();
}
```

## ğŸ“ æœ€ä½³å®è·µæ€»ç»“

### è®¾è®¡åŸåˆ™
1. **ä¼˜å…ˆä½¿ç”¨é«˜çº§æŠ½è±¡**: ä¼˜å…ˆä½¿ç”¨`std::async`ã€`std::future`è€Œéç›´æ¥æ“ä½œçº¿ç¨‹
2. **é¿å…æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹**: ä½¿ç”¨çº¿ç¨‹æ± æˆ–ä»»åŠ¡è°ƒåº¦å™¨
3. **æœ€å°åŒ–å…±äº«çŠ¶æ€**: è®¾è®¡æ— å…±äº«æˆ–æœ€å°å…±äº«çš„æ¶æ„
4. **ä½¿ç”¨RAIIç®¡ç†èµ„æº**: ç¡®ä¿å¼‚å¸¸å®‰å…¨

### æ€§èƒ½å‡†åˆ™
1. **äº†è§£ç¡¬ä»¶ç‰¹æ€§**: CPUæ ¸å¿ƒæ•°ã€ç¼“å­˜å¤§å°ã€å†…å­˜å¸¦å®½
2. **é¿å…è™šå‡å…±äº«**: ä½¿ç”¨ç¼“å­˜è¡Œå¯¹é½
3. **é€‰æ‹©åˆé€‚çš„åŒæ­¥åŸè¯­**: è¯»å¤šå†™å°‘ç”¨è¯»å†™é”ï¼Œç®€å•æ“ä½œç”¨åŸå­
4. **æ‰¹é‡å¤„ç†å‡å°‘åŒæ­¥**: åˆå¹¶å°æ“ä½œå‡å°‘é”ç«äº‰

### è°ƒè¯•å»ºè®®
1. **ä½¿ç”¨ThreadSanitizer**: ç¼–è¯‘æ—¶æ·»åŠ `-fsanitize=thread`
2. **æ·»åŠ è°ƒè¯•é’©å­**: å¦‚ä¸Šé¢çš„`ThreadAwareDebugger`
3. **è®°å½•çº¿ç¨‹æ´»åŠ¨**: å…³é”®æ“ä½œæ·»åŠ æ—¥å¿—
4. **å‹åŠ›æµ‹è¯•**: é«˜å¹¶å‘åœºæ™¯æµ‹è¯•

### ä»£ç è´¨é‡
1. **ç¼–å†™çº¿ç¨‹å®‰å…¨æ–‡æ¡£**: æ˜ç¡®å“ªäº›å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„
2. **å•å…ƒæµ‹è¯•å¹¶å‘ä»£ç **: ä½¿ç”¨Google Testç­‰æ¡†æ¶
3. **ä»£ç å®¡æŸ¥é‡ç‚¹å…³æ³¨**: åŒæ­¥åŸè¯­çš„ä½¿ç”¨
4. **æŒç»­é›†æˆæ£€æŸ¥**: é›†æˆThreadSanitizeråˆ°CI

---

*æœ¬æ–‡æ¡£åŸºäºC++20æ ‡å‡†ç¼–å†™ï¼Œå»ºè®®ç»“åˆæœ€æ–°C++æ ‡å‡†æ–‡æ¡£å­¦ä¹ ã€‚*
*å®é™…å¼€å‘ä¸­è¯·æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„å¹¶å‘æ¨¡å‹å’Œå·¥å…·ã€‚*
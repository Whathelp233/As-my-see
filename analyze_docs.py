#!/usr/bin/env python3
"""
æ–‡æ¡£è´¨é‡åˆ†æå·¥å…·
åˆ†æAs-my-seeçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£è´¨é‡
"""

import os
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

class DocumentAnalyzer:
    def __init__(self, docs_dir):
        self.docs_dir = Path(docs_dir)
        self.report_file = self.docs_dir.parent / f"document_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        # ç»Ÿè®¡
        self.stats = {
            'total': 0,
            'good': 0,
            'shallow': 0,
            'duplicate': 0
        }
        
        self.shallow_docs = []
        self.duplicate_docs = []
        self.category_stats = defaultdict(lambda: {'total': 0, 'shallow': 0})
    
    def analyze_document(self, filepath):
        """åˆ†æå•ä¸ªæ–‡æ¡£"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
                
            # åŸºæœ¬ç»Ÿè®¡
            line_count = len(lines)
            
            # æ£€æŸ¥å†…å®¹
            has_titles = any(line.strip().startswith('#') for line in lines)
            has_code = '```' in content
            has_lists = any(line.strip().startswith('-') or line.strip().startswith('*') for line in lines)
            
            # åˆ¤æ–­è´¨é‡
            if line_count < 30:
                quality = 'ğŸ”´ è¿‡æµ…'
                needs_improvement = True
            elif line_count < 100:
                quality = 'ğŸŸ¡ ä¸€èˆ¬'
                needs_improvement = True
            elif not has_titles:
                quality = 'ğŸŸ¡ æ— ç»“æ„'
                needs_improvement = True
            else:
                quality = 'ğŸŸ¢ è‰¯å¥½'
                needs_improvement = False
            
            return {
                'path': str(filepath.relative_to(self.docs_dir)),
                'lines': line_count,
                'quality': quality,
                'needs_improvement': needs_improvement,
                'has_titles': has_titles,
                'has_code': has_code,
                'has_lists': has_lists
            }
            
        except Exception as e:
            print(f"åˆ†ææ–‡æ¡£å¤±è´¥: {filepath}, é”™è¯¯: {e}")
            return None
    
    def find_duplicates(self, files):
        """æŸ¥æ‰¾é‡å¤æ–‡æ¡£"""
        duplicates = []
        seen = {}
        
        for filepath in files:
            filename = filepath.name
            # æ£€æŸ¥æ˜¯å¦å¸¦æ•°å­—åç¼€
            if '_' in filename and filename.endswith('.md'):
                base_name = filename[:-3]  # å»æ‰.md
                parts = base_name.split('_')
                if parts[-1].isdigit():
                    # å¯èƒ½æ˜¯é‡å¤æ–‡æ¡£
                    original_name = '_'.join(parts[:-1]) + '.md'
                    if original_name in seen:
                        duplicates.append({
                            'path': str(filepath.relative_to(self.docs_dir)),
                            'original': seen[original_name]
                        })
                else:
                    seen[filename] = str(filepath.relative_to(self.docs_dir))
            else:
                seen[filename] = str(filepath.relative_to(self.docs_dir))
        
        return duplicates
    
    def run_analysis(self):
        """è¿è¡Œåˆ†æ"""
        print("å¼€å§‹åˆ†ææ–‡æ¡£...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
        all_files = list(self.docs_dir.rglob('*.md'))
        self.stats['total'] = len(all_files)
        
        # æŸ¥æ‰¾é‡å¤æ–‡æ¡£
        self.duplicate_docs = self.find_duplicates(all_files)
        self.stats['duplicate'] = len(self.duplicate_docs)
        
        # æŒ‰ç›®å½•åˆ†æ
        report_content = []
        report_content.append("# ğŸ“Š æ–‡æ¡£è´¨é‡åˆ†ææŠ¥å‘Š")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append(f"åˆ†æç›®å½•: {self.docs_dir}")
        report_content.append(f"æ–‡æ¡£æ€»æ•°: {self.stats['total']}")
        report_content.append("")
        
        # æŒ‰ç›®å½•åˆ†æ
        for category_dir in sorted(self.docs_dir.iterdir()):
            if category_dir.is_dir():
                category_name = category_dir.name
                category_files = list(category_dir.rglob('*.md'))
                
                if not category_files:
                    continue
                
                report_content.append(f"## ğŸ“ {category_name}")
                report_content.append("")
                report_content.append("| æ–‡æ¡£ | è¡Œæ•° | è´¨é‡ | å»ºè®® |")
                report_content.append("|------|------|------|------|")
                
                category_shallow = 0
                
                for filepath in sorted(category_files):
                    result = self.analyze_document(filepath)
                    if not result:
                        continue
                    
                    self.stats['total'] += 1
                    self.category_stats[category_name]['total'] += 1
                    
                    if result['needs_improvement']:
                        self.stats['shallow'] += 1
                        category_shallow += 1
                        self.category_stats[category_name]['shallow'] += 1
                        self.shallow_docs.append({
                            'path': result['path'],
                            'lines': result['lines'],
                            'quality': result['quality']
                        })
                    else:
                        self.stats['good'] += 1
                    
                    # ç”Ÿæˆå»ºè®®
                    suggestion = "ä¿æŒ"
                    if result['lines'] < 30:
                        suggestion = "éœ€è¦å¤§å¹…æ‰©å……"
                    elif result['lines'] < 100:
                        suggestion = "å»ºè®®è¡¥å……å†…å®¹"
                    elif not result['has_titles']:
                        suggestion = "æ·»åŠ æ ‡é¢˜ç»“æ„"
                    elif not result['has_code'] and 'ä»£ç ' in result['path']:
                        suggestion = "æ·»åŠ ä»£ç ç¤ºä¾‹"
                    
                    report_content.append(f"| {result['path']} | {result['lines']} | {result['quality']} | {suggestion} |")
                
                report_content.append("")
                report_content.append(f"**ç»Ÿè®¡**: {len(category_files)} ä¸ªæ–‡æ¡£ï¼Œå…¶ä¸­ {category_shallow} ä¸ªéœ€è¦ä¼˜åŒ–")
                report_content.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        report_content.append("# ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        report_content.append("")
        report_content.append(f"- **æ€»æ–‡æ¡£æ•°**: {self.stats['total']}")
        report_content.append(f"- **è´¨é‡è‰¯å¥½**: {self.stats['good']} ({self.stats['good']/self.stats['total']*100:.1f}%)")
        report_content.append(f"- **éœ€è¦ä¼˜åŒ–**: {self.stats['shallow']} ({self.stats['shallow']/self.stats['total']*100:.1f}%)")
        report_content.append(f"- **é‡å¤æ–‡æ¡£**: {self.stats['duplicate']} ({self.stats['duplicate']/self.stats['total']*100:.1f}%)")
        report_content.append("")
        
        # éœ€è¦ä¼˜åŒ–çš„æ–‡æ¡£
        if self.shallow_docs:
            report_content.append("# ğŸ”§ éœ€è¦ä¼˜åŒ–çš„æ–‡æ¡£")
            report_content.append("")
            report_content.append(f"ä»¥ä¸‹ {len(self.shallow_docs)} ä¸ªæ–‡æ¡£éœ€è¦ä¼˜åŒ–:")
            report_content.append("")
            
            # æŒ‰è¡Œæ•°æ’åºï¼Œæœ€å°‘çš„ä¼˜å…ˆ
            sorted_shallow = sorted(self.shallow_docs, key=lambda x: x['lines'])
            for doc in sorted_shallow[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                report_content.append(f"- {doc['path']} ({doc['lines']} è¡Œ) - {doc['quality']}")
            
            if len(self.shallow_docs) > 20:
                report_content.append(f"- ... è¿˜æœ‰ {len(self.shallow_docs) - 20} ä¸ª")
            report_content.append("")
        
        # é‡å¤æ–‡æ¡£
        if self.duplicate_docs:
            report_content.append("# ğŸ”„ é‡å¤æ–‡æ¡£")
            report_content.append("")
            report_content.append(f"ä»¥ä¸‹ {len(self.duplicate_docs)} ä¸ªæ–‡æ¡£å¯èƒ½æ˜¯é‡å¤çš„:")
            report_content.append("")
            
            for dup in self.duplicate_docs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report_content.append(f"- {dup['path']} (å¯èƒ½æ˜¯ {dup['original']} çš„é‡å¤)")
            
            if len(self.duplicate_docs) > 10:
                report_content.append(f"- ... è¿˜æœ‰ {len(self.duplicate_docs) - 10} ä¸ª")
            report_content.append("")
        
        # å»ºè®®
        report_content.append("# ğŸ¯ ä¼˜åŒ–å»ºè®®")
        report_content.append("")
        report_content.append("## ç«‹å³è¡ŒåŠ¨")
        report_content.append("")
        report_content.append("1. **ä¼˜å…ˆä¼˜åŒ–æ ¸å¿ƒæ–‡æ¡£**: é€‰æ‹©3-5ä¸ªæœ€é‡è¦çš„æŠ€æœ¯æ–‡æ¡£è¿›è¡Œæ·±åº¦ä¼˜åŒ–")
        report_content.append("2. **æ¸…ç†é‡å¤æ–‡æ¡£**: åˆ é™¤æˆ–åˆå¹¶å¸¦ `_1`, `_2` ç­‰åç¼€çš„é‡å¤æ–‡æ¡£")
        report_content.append("3. **å»ºç«‹æ–‡æ¡£æ ‡å‡†**: å‚è€ƒè´¨é‡è‰¯å¥½çš„æ–‡æ¡£ï¼Œåˆ¶å®šæ–‡æ¡£ç¼–å†™è§„èŒƒ")
        report_content.append("")
        
        report_content.append("## è‡ªåŠ¨åŒ–å»ºè®®")
        report_content.append("")
        report_content.append("1. **è®¾ç½®å®šæ—¶åˆ†æ**: æ¯å‘¨è‡ªåŠ¨è¿è¡Œæ–‡æ¡£è´¨é‡åˆ†æ")
        report_content.append("2. **è‡ªåŠ¨å¤‡ä»½**: ä¼˜åŒ–å‰è‡ªåŠ¨å¤‡ä»½åŸæ–‡æ¡£")
        report_content.append("3. **è´¨é‡ç›‘æ§**: è®¾ç½®æ–‡æ¡£è´¨é‡é˜ˆå€¼ï¼Œè‡ªåŠ¨æé†’")
        report_content.append("")
        
        # å†™å…¥æŠ¥å‘Š
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        print(f"\n=== æ–‡æ¡£è´¨é‡åˆ†æå®Œæˆ ===")
        print(f"æŠ¥å‘Šæ–‡ä»¶: {self.report_file}")
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {self.stats['total']}")
        print(f"  è´¨é‡è‰¯å¥½: {self.stats['good']} ({self.stats['good']/self.stats['total']*100:.1f}%)")
        print(f"  éœ€è¦ä¼˜åŒ–: {self.stats['shallow']} ({self.stats['shallow']/self.stats['total']*100:.1f}%)")
        print(f"  é‡å¤æ–‡æ¡£: {self.stats['duplicate']} ({self.stats['duplicate']/self.stats['total']*100:.1f}%)")
        
        if self.shallow_docs:
            print(f"\nğŸ”§ éœ€è¦ä¼˜åŒ–çš„æ–‡æ¡£ï¼ˆå‰10ä¸ªï¼ŒæŒ‰è¡Œæ•°æ’åºï¼‰:")
            sorted_shallow = sorted(self.shallow_docs, key=lambda x: x['lines'])
            for doc in sorted_shallow[:10]:
                print(f"  - {doc['path']} ({doc['lines']} è¡Œ)")

def main():
    docs_dir = "/root/.openclaw/workspace/As-my-see/docs"
    
    if not os.path.exists(docs_dir):
        print(f"é”™è¯¯: æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
        sys.exit(1)
    
    analyzer = DocumentAnalyzer(docs_dir)
    analyzer.run_analysis()

if __name__ == "__main__":
    main()